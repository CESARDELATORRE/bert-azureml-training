{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3a: Training Workflow using Azure Machine Learning Service\n",
    "\n",
    "This notebook is showcases Azure Machine Learning Pipelines. To learn more about AML Pipelines, use the following resources:\n",
    "\n",
    "- https://aka.ms/pl-concept (What are Azure Machine Learning pipelines?)\n",
    "- https://aka.ms/pl-first-pipeline (Create and run machine learning pipelines)\n",
    "- https://aka.ms/pl-notebooks (AML Pipelines sample notebooks)\n",
    "\n",
    "This notebook creates a simple training pipelines which trains the model, evaluates the model, and registers the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "This notebook is designed to be run in Azure ML Notebook VM. See [readme](https://github.com/microsoft/bert-stack-overflow/blob/master/README.md) file for instructions on how to create Notebook VM and open this notebook in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Azure Machine Learning Python SDK version\n",
    "\n",
    "This tutorial requires version 1.0.69 or higher. Let's check the version of the SDK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "\n",
    "print(\"Azure Machine Learning Python SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stackoverflow Question Tagging Problem \n",
    "We will use a simple (dummy) pipeline for now and leave creation of a pipeline using the exact training problem that you used earlier in the workshop as an exercise. Refer to [training notebook](../1-Training/AzureServiceClassifier_Training.ipynb) for details on how you can use an `EstimatorStep` and `PythonScriptStep` to build a train-evaluate-register pipeline. You may also refer to [this repo](https://github.com/microsoft/bert-stack-overflow/), and specifically [this file](https://github.com/microsoft/bert-stack-overflow/blob/master/3-ML-Ops/train-and-register-model.py) for creating a pipeline.\n",
    "\n",
    "Here, instead of using the script runs as is, we are going to use a AML Pipeline step to do the training.\n",
    "\n",
    "<img src=\"./images/pipeline.png\" alt=\"pipeline\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to your workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "workspace = Workspace.from_config()\n",
    "print('Workspace name: ' + workspace.name, \n",
    "      'Azure region: ' + workspace.location, \n",
    "      'Subscription id: ' + workspace.subscription_id, \n",
    "      'Resource group: ' + workspace.resource_group, sep = '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get or create your Compute Target\n",
    "\n",
    "You have already creared a compute target (`v100cluster` or `p100cluster`) for training. It is time to get that compute cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# replace gpu-cluster with your own training cluster name\n",
    "aml_compute_target = \"gpu-cluster\"\n",
    "\n",
    "try:\n",
    "    aml_compute = AmlCompute(workspace, aml_compute_target)\n",
    "    print(\"Found existing compute target.\")\n",
    "except ComputeTargetException:\n",
    "    print(\"creating new compute target\")\n",
    "    \n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"Standard_NC12s_v3\",\n",
    "                                                                min_nodes = 1, \n",
    "                                                                max_nodes = 2)    \n",
    "    aml_compute = ComputeTarget.create(ws, aml_compute_target, provisioning_config)\n",
    "    aml_compute.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "print(\"Azure Machine Learning Compute attached\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get default Datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Datastore \n",
    "\n",
    "def_blob_store = Datastore(workspace, \"workspaceblobstore\")\n",
    "def_blob_store.upload_files([\"model/model.pkl\"], target_path=\"model\", overwrite=False)\n",
    "def_blob_store.upload_files([\"data/data.csv\"], target_path=\"data\", overwrite=False)\n",
    "\n",
    "print(\"File uploaded to default datastore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Experiment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment_name = 'azml-classifier-using-pipelines' \n",
    "experiment = Experiment(workspace, name=experiment_name)\n",
    "print(\"Experiment object created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Training Step\n",
    "A Step in a pipeline is a unit of execution. Step typically needs a target of execution (compute target), a script to execute, and may require script arguments and inputs, and can produce outputs. The step also could take a number of other parameters. Azure Machine Learning Pipelines provides the following common built-in Steps (among others). Steps are [documented here](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-steps/azureml.pipeline.steps?view=azure-ml-py).\n",
    "\n",
    "- PythonScriptStep: Adds a step to run a Python script in a Pipeline.\n",
    "- DataTransferStep: Transfers data between Azure Blob and Data Lake accounts.\n",
    "- DatabricksStep: Adds a DataBricks notebook as a step in a Pipeline.\n",
    "- HyperDriveStep: Creates a Hyper Drive step for Hyper Parameter Tuning in a Pipeline.\n",
    "- EstimatorStep: Adds a step to run Estimator in a Pipeline.\n",
    "- AutoMLStep: Creates a AutoML step in a Pipeline.\n",
    "\n",
    "The following code will create a *PythonScriptStep* to be executed in the Azure Machine Learning Compute we created above using `train.py`, one of the files already made available in the source_directory.\n",
    "\n",
    "A *PythonScriptStep* is a basic, built-in step to run a Python Script on a compute target. It takes a script name and optionally other parameters like arguments for the script, compute target, inputs and outputs. If no compute target is specified, default compute target for the workspace is used. You can also use a RunConfiguration to specify requirements for the PythonScriptStep, such as conda dependencies and docker image.\n",
    "\n",
    "> The best practice is to use separate folders for scripts and its dependent files for each step and specify that folder as the source_directory for the step. This helps reduce the size of the snapshot created for the step (only the specific folder is snapshotted). Since changes in any files in the source_directory would trigger a re-upload of the snapshot, this helps keep the reuse of the step when there are no changes in the source_directory of the step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a runconfig\n",
    "Need to create a specific [runconfig](https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.runconfig.runconfiguration?view=azure-ml-py) for evaluation. Runconfig represents configuration for experiment runs targeting different compute targets in Azure Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Train Step\n",
    "Create train step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.pipeline.core import PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "input_data = DataReference(\n",
    "    datastore=def_blob_store,\n",
    "    data_reference_name=\"train_data\",\n",
    "    path_on_datastore=\"data/data.csv\")\n",
    "\n",
    "model_dir = PipelineData(\"model_dir\", datastore=def_blob_store)\n",
    "\n",
    "train_step = PythonScriptStep(\n",
    "                name=\"Train Model\",\n",
    "                source_directory='scripts/training',\n",
    "                script_name=\"train_model.py\", \n",
    "                arguments=[\"--input_data\", input_data, \"--model_dir\", model_dir],\n",
    "                inputs=[input_data],\n",
    "                outputs=[model_dir],\n",
    "                compute_target=aml_compute, \n",
    "                allow_reuse=True)\n",
    "print(\"train_step created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Evaluate Step\n",
    "Now, let's create a step to evaluate the model created using the step above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Step using the above runconfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_model = DataReference(\n",
    "    datastore=def_blob_store,\n",
    "    data_reference_name=\"current_model\",\n",
    "    path_on_datastore=\"model/model.pkl\")\n",
    "\n",
    "eval_result = PipelineData(\"eval_result\", datastore=def_blob_store)\n",
    "\n",
    "evaluate_step = PythonScriptStep(\n",
    "                    name=\"Evaluate Model\",\n",
    "                    source_directory=\"scripts/evaluate\",\n",
    "                    script_name=\"evaluate.py\",\n",
    "                    arguments=[\"--existing_model\", existing_model, \"--model_dir\", model_dir, \"--evaluate_result\", eval_result],\n",
    "                    inputs=[existing_model, model_dir],\n",
    "                    outputs=[eval_result],\n",
    "                    compute_target=aml_compute, \n",
    "                    allow_reuse=True)\n",
    "\n",
    "print(\"evaluate_step created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Register Step\n",
    "Once the model is evaluated, we can register the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_step = PythonScriptStep(\n",
    "                    name=\"Register Model\",\n",
    "                    source_directory=\"scripts/register\",\n",
    "                    script_name=\"register.py\",\n",
    "                    compute_target=aml_compute,\n",
    "                    arguments=[\"--eval_result\", eval_result],\n",
    "                    inputs=[eval_result],\n",
    "                    allow_reuse=True)\n",
    "print(\"register_step created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the pipeline\n",
    "Once we have the steps (or steps collection), we can build the [pipeline](https://docs.microsoft.com/en-us/python/api/azureml-pipeline-core/azureml.pipeline.core.pipeline.pipeline?view=azure-ml-py). By deafult, all these steps will run in parallel unless there is explicit data dependency (as in the case of this pipeline). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [train_step, evaluate_step, register_step]\n",
    "\n",
    "from azureml.pipeline.core import Pipeline\n",
    "\n",
    "pipeline = Pipeline(workspace=workspace, steps=steps)\n",
    "print (\"Pipeline is built\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit a pipeline run \n",
    "We can now submit the pipeline using the experiment object.\n",
    ">If `regenerate_outputs` is set to True, a new submit will always force generation of all step outputs, and disallow data reuse for any step of this run. Once this run is complete, however, subsequent runs may reuse the results of this run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run = experiment.submit(pipeline, regenerate_outputs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the current status of the run and stream the logs from within the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(pipeline_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pipeline_run.cancel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python [conda env:cli_dev]",
   "language": "python",
   "name": "conda-env-cli_dev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
